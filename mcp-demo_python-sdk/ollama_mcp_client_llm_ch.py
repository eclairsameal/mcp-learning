import asyncio
import json
import ollama
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

"""
è®“ LLM ç›´æ¥æ±ºå®šè¦å‘¼å«å“ªå€‹å·¥å…·
å¯ä»¥å…ˆæŠŠå¯ç”¨å·¥å…·åˆ—è¡¨å‚³çµ¦ LLMï¼Œç„¶å¾Œå• LLMã€Œé€™å€‹ prompt è¦å‘¼å«å“ªå€‹å·¥å…·ï¼Ÿã€ï¼Œè®“ LLM è¿”å›å·¥å…·åå’Œåƒæ•¸ï¼Œé€™æ¨£å°±å®Œå…¨ä¸ç”¨å¯«æ­»äº†ã€‚
"""


async def main():
    server_params = StdioServerParameters(
        command="python",
        args=["mcp_server.py"],
    )

    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()

            prompts = [
                "è«‹å¹«æˆ‘æŸ¥ä»Šå¤©æ±äº¬çš„å¤©æ°£",
                "å¹«æˆ‘è¨ˆç®— 3*7+2 çš„å€¼",
                "è«‹å•å¤§é˜ªå¤©æ°£å¦‚ä½•",
                "è¨ˆç®— 15/3+4"
            ]

            for prompt in prompts:
                print(f"\nğŸ’¬ LLM Prompt: {prompt}")

                # ==== å…ˆè®“ LLM æ±ºå®šå·¥å…·èˆ‡åƒæ•¸ ====
                tool_decision_prompt = f"""
                ä½ æ˜¯åŠ©æ‰‹ï¼Œæ ¹æ“šä¸‹é¢ä½¿ç”¨è€…çš„æŒ‡ä»¤é¸æ“‡è¦å‘¼å«çš„å·¥å…·ä»¥åŠåƒæ•¸ã€‚
                å¯ç”¨å·¥å…·ï¼š
                1. get_weather(city: str)
                2. calculate(expression: str)

                è«‹ä»¥ JSON æ ¼å¼å›è¦†ï¼š
                {{
                    "tool": "å·¥å…·åç¨±",
                    "args": {{}}
                }}

                æŒ‡ä»¤: {prompt}
                """
                decision_resp = ollama.generate(
                    model="llama3.2:latest",
                    prompt=tool_decision_prompt
                )
                decision_text = getattr(decision_resp, "response", str(decision_resp))

                try:
                    decision_json = json.loads(decision_text)
                    tool_name = decision_json["tool"]
                    args = decision_json["args"]
                except Exception as e:
                    print(f"âš ï¸ ç„¡æ³•è§£æ LLM æ±ºå®šçµæœ: {decision_text}")
                    continue

                # ==== å‘¼å« MCP å·¥å…· ====
                tool_res = await session.call_tool(name=tool_name, arguments=args)
                tool_output = tool_res.content[0].text if tool_res.content else str(tool_res)
                print(f"ğŸ›  å·¥å…·çµæœ: {tool_output}")

                # ==== ä½¿ç”¨ Ollama æ•´ç†å›è¦† ====
                response = ollama.generate(
                    model="llama3.2:latest",
                    prompt=f"å·¥å…·çµæœ: {tool_output}\nè«‹å¹«æˆ‘æ•´ç†æˆå®Œæ•´å›ç­”",
                )
                llm_text = getattr(response, "response", str(response))
                print("ğŸ¤– LLM å›è¦†:", llm_text)


asyncio.run(main())
